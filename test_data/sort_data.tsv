200115	"Insertion sort Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages: When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort. Insertion sort iterates, consuming one input element each repetition, and growing a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the"	"Insertion sort"
200116	"sorted list, and inserts it there. It repeats until no input elements remain. Sorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position. The resulting array"	"Insertion sort"
200117	"after ""k"" iterations has the property where the first ""k"" + 1 entries are sorted (""+1"" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result: becomes with each element greater than ""x"" copied to the right as it is compared against ""x"". The most common variant of insertion sort, which operates on arrays, can be described as follows: Pseudocode of the complete algorithm follows, where the arrays are zero-based: The outer loop runs over all the elements except"	"Insertion sort"
200118	"the first one, because the single-element prefix codice_1 is trivially sorted, so the invariant that the first codice_2 entries are sorted is true from the start. The inner loop moves element codice_3 to its correct place so that after the loop, the first codice_4 elements are sorted. Note that the codice_5-operator in the test must use short-circuit evaluation, otherwise the test might get stuck with an array bounds error, when codice_6 and it tries to evaluate codice_7 (i.e. accessing codice_8 fails). After expanding the codice_9 operation in-place as codice_10 (where codice_11 is a temporary variable), a slightly faster version can"	"Insertion sort"
200119	"be produced that moves codice_3 to its position in one go and only performs one assignment in the inner loop body: The new inner loop shifts elements to the right to clear a spot for codice_13. The algorithm can also be implemented in a recursive way. The recursion just replaces the outer loop, calling itself and storing successively smaller values of ""n"" on the stack until ""n"" equals 0, where the function then returns back up the call chain to execute the code after each recursive call starting with ""n"" equal to 1, with ""n"" increasing by 1 as each"	"Insertion sort"
200120	"instance of the function returns to the prior instance. The initial call would be ""insertionSortR(A, length(A)-1)"" . The best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., O(""n"")). During each iteration, the first remaining element of the input is only compared with the right-most element of the sorted subsection of the array. The simplest worst case input is an array sorted in reverse order. The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it."	"Insertion sort"
200121	"In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. This gives insertion sort a quadratic running time (i.e., O(""n"")). The average case is also quadratic, which makes insertion sort impractical for sorting large arrays. However, insertion sort is one of the fastest algorithms for sorting very small arrays, even faster than quicksort; indeed, good quicksort implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but"	"Insertion sort"
200122	"is commonly around ten. Example: The following table shows the steps for sorting the sequence {3, 7, 4, 9, 5, 2, 6, 1}. In each step, the key under consideration is underlined. The key that was moved (or left in place because it was biggest yet considered) in the previous step is marked with an asterisk. Insertion sort is very similar to selection sort. As in selection sort, after ""k"" passes through the array, the first ""k"" elements are in sorted order. However, the fundamental difference between the two algorithms is that for selection sort these are the ""k"" smallest"	"Insertion sort"
200123	"elements of the unsorted input, while in insertion sort they are simply the first ""k"" elements of the input. The primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the ""k""+1th element is greater than the ""k""th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. On average (assuming the"	"Insertion sort"
200124	"rank of the ""k""+1th element rank is random), insertion sort will require comparing and shifting half of the previous ""k"" elements, meaning insertion sort will perform about half as many comparisons as selection sort on average. In the worst case for insertion sort (when the input array is reverse-sorted), insertion sort performs just as many comparisons as selection sort. However, a disadvantage of insertion sort over selection sort is that it requires more writes due to the fact that, on each iteration, inserting the ""k""+1th element into the sorted portion of the array requires many element swaps to shift all"	"Insertion sort"
200125	"of the following elements, while only a single swap is required for each iteration of selection sort. In general, insertion sort will write to the array O(""n"") times, whereas selection sort will write only O() times. For this reason selection sort may be preferable in cases where writing to memory is significantly more expensive than reading, such as with EEPROM or flash memory. While some divide-and-conquer algorithms such as quicksort and mergesort outperform insertion sort for larger arrays, non-recursive sorting algorithms such as insertion sort or selection sort are generally faster for very small arrays (the exact size varies by"	"Insertion sort"
200126	"environment and implementation, but is typically between seven and fifty elements). Therefore, a useful optimization in the implementation of those algorithms is a hybrid approach, using the simpler algorithm when the array has been divided to a small size. D.L. Shell made substantial improvements to the algorithm; the modified version is called Shell sort. The sorting algorithm compares elements separated by a distance that decreases on each pass. Shell sort has distinctly improved running times in practical work, with two simple variants requiring O(""n"") and O(""n"") running time. If the cost of comparisons exceeds the cost of swaps, as is"	"Insertion sort"
200127	"the case for example with string keys stored by reference or with human interaction (such as choosing one of a pair displayed side-by-side), then using ""binary insertion sort"" may yield better performance. Binary insertion sort employs a binary search to determine the correct location to insert new elements, and therefore performs ⌈log(""n"")⌉ comparisons in the worst case, which is O(""n"" log ""n""). The algorithm as a whole still has a running time of O(""n"") on average because of the series of swaps required for each insertion. The number of swaps can be reduced by calculating the position of multiple elements"	"Insertion sort"
200128	"before moving them. For example, if the target position of two elements is calculated before they are moved into the right position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to merge sort. A variant named ""binary merge sort"" uses a ""binary insertion sort"" to sort groups of 32 elements, followed by a final sort using merge sort. It combines the speed of insertion sort on small data sets with the speed of merge sort on large data sets. To avoid having to make a series of"	"Insertion sort"
200129	"swaps for each insertion, the input could be stored in a linked list, which allows elements to be spliced into or out of the list in constant-time when the position in the list is known. However, searching a linked list requires sequentially following the links to the desired position: a linked list does not have random access, so it cannot use a faster method such as binary search. Therefore, the running time required for searching is O(""n"") and the time for sorting is O(""n""). If a more sophisticated data structure (e.g., heap or binary tree) is used, the time required"	"Insertion sort"
200130	"for searching and insertion can be reduced significantly; this is the essence of heap sort and binary tree sort. In 2006 Bender, Martin Farach-Colton, and Mosteiro published a new variant of insertion sort called ""library sort"" or ""gapped insertion sort"" that leaves a small number of unused spaces (i.e., ""gaps"") spread throughout the array. The benefit is that insertions need only shift elements over until a gap is reached. The authors show that this sorting algorithm runs with high probability in O(""n"" log ""n"") time. If a skip list is used, the insertion time is brought down to O(log ""n""),"	"Insertion sort"
200131	"and swaps are not needed because the skip list is implemented on a linked list structure. The final running time for insertion would be O(""n"" log ""n""). ""List insertion sort"" is a variant of insertion sort. It reduces the number of movements. If the items are stored in a linked list, then the list can be sorted with O(1) additional space. The algorithm starts with an initially empty (and therefore trivially sorted) list. The input items are taken off the list one at a time, and then inserted in the proper place in the sorted list. When the input list"	"Insertion sort"
200132	"is empty, the sorted list has the desired result. The algorithm below uses a trailing pointer for the insertion into the sorted list. A simpler recursive method rebuilds the list each time (rather than splicing) and can use O(""n"") stack space. Insertion sort Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages: When people manually sort cards in a bridge hand, most use a"	"Insertion sort"
14235781	"Bubble sort Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list, compares adjacent pairs and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller or larger elements ""bubble"" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems even when compared to insertion sort. Bubble sort can be practical if the input is in mostly"	"Bubble sort"
14235782	"sorted order with some out-of-order elements nearly in position. Bubble sort has a worst-case and average complexity of ""О""(""n""), where ""n"" is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often ""O""(""n"" log ""n""). Even other ""О""(""n"") sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. Therefore, bubble sort is not a practical sorting algorithm. The only significant advantage that bubble sort has over most other algorithms, even quicksort, but not insertion sort, is that the ability to detect that the list is"	"Bubble sort"
14235783	"sorted efficiently is built into the algorithm. When the list is already sorted (best-case), the complexity of bubble sort is only ""O""(""n""). By contrast, most other algorithms, even those with better average-case complexity, perform their entire sorting process on the set and thus are more complex. However, not only does insertion sort share this advantage, but it also performs better on a list that is substantially sorted (having a small number of inversions). Bubble sort should be avoided in the case of large collections. It will not be efficient in the case of a reverse-ordered collection. The distance and direction"	"Bubble sort"
14235784	"that elements must move during the sort determine bubble sort's performance because elements move in different directions at different speeds. An element that must move toward the end of the list can move quickly because it can take part in successive swaps. For example, the largest element in the list will win every swap, so it moves to its sorted position on the first pass even if it starts near the beginning. On the other hand, an element that must move toward the beginning of the list cannot move faster than one step per pass, so elements move toward the"	"Bubble sort"
14235785	"beginning very slowly. If the smallest element is at the end of the list, it will take passes to move it to the beginning. This has led to these types of elements being named rabbits and turtles, respectively, after the characters in Aesop's fable of The Tortoise and the Hare. Various efforts have been made to eliminate turtles to improve upon the speed of bubble sort. Cocktail sort is a bi-directional bubble sort that goes from beginning to end, and then reverses itself, going end to beginning. It can move turtles fairly well, but it retains ""O(n)"" worst-case complexity. Comb"	"Bubble sort"
14235786	"sort compares elements separated by large gaps, and can move turtles extremely quickly before proceeding to smaller and smaller gaps to smooth out the list. Its average speed is comparable to faster algorithms like quicksort. Take an array of numbers "" 5 1 4 2 8"", and sort the array from lowest number to greatest number using bubble sort. In each step, elements written in bold are being compared. Three passes will be required. Now, the array is already sorted, but the algorithm does not know if it is completed. The algorithm needs one whole pass without any swap to"	"Bubble sort"
14235787	"know it is sorted. The algorithm can be expressed as (0-based array): The bubble sort algorithm can be easily optimized by observing that the ""n""-th pass finds the ""n""-th largest element and puts it into its final place. So, the inner loop can avoid looking at the last ""n"" − 1 items when running for the ""n""-th time: More generally, it can happen that more than one element is placed in their final position on a single pass. In particular, after every pass, all elements after the last swap are sorted, and do not need to be checked again. This"	"Bubble sort"
14235788	"allows us to skip over a lot of the elements, resulting in about a worst case 50% improvement in comparison count (though no improvement in swap counts), and adds very little complexity because the new code subsumes the ""swapped"" variable: To accomplish this in pseudocode we write the following: Alternate modifications, such as the cocktail shaker sort attempt to improve on the bubble sort performance while keeping the same idea of repeatedly comparing and swapping adjacent items. Although bubble sort is one of the simplest sorting algorithms to understand and implement, its ""O""(""n"") complexity means that its efficiency decreases dramatically"	"Bubble sort"
14235789	"on lists of more than a small number of elements. Even among simple ""O""(""n"") sorting algorithms, algorithms like insertion sort are usually considerably more efficient. Due to its simplicity, bubble sort is often used to introduce the concept of an algorithm, or a sorting algorithm, to introductory computer science students. However, some researchers such as Owen Astrachan have gone to great lengths to disparage bubble sort and its continued popularity in computer science education, recommending that it no longer even be taught. The Jargon File, which famously calls bogosort ""the archetypical [sic] perversely awful algorithm"", also calls bubble sort ""the"	"Bubble sort"
14235790	"generic bad algorithm"". Donald Knuth, in ""The Art of Computer Programming"", concluded that ""the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems"", some of which he then discusses. Bubble sort is asymptotically equivalent in running time to insertion sort in the worst case, but the two algorithms differ greatly in the number of swaps necessary. Experimental results such as those of Astrachan have also shown that insertion sort performs considerably better even on random lists. For these reasons many modern algorithm textbooks avoid using"	"Bubble sort"
14235791	"the bubble sort algorithm in favor of insertion sort. Bubble sort also interacts poorly with modern CPU hardware. It produces at least twice as many writes as insertion sort, twice as many cache misses, and asymptotically more branch mispredictions. Experiments by Astrachan sorting strings in Java show bubble sort to be roughly one-fifth as fast as an insertion sort and 70% as fast as a selection sort. In computer graphics bubble sort is popular for its capability to detect a very small error (like swap of just two elements) in almost-sorted arrays and fix it with just linear complexity (2""n"")."	"Bubble sort"
14235792	"For example, it is used in a polygon filling algorithm, where bounding lines are sorted by their ""x"" coordinate at a specific scan line (a line parallel to the ""x"" axis) and with incrementing ""y"" their order changes (two elements are swapped) only at intersections of two lines. Bubble sort is a stable sort algorithm, like insertion sort. Bubble sort has been occasionally referred to as a ""sinking sort"". For example, in Donald Knuth's ""The Art of Computer Programming"", Volume 3: ""Sorting and Searching"" he states in section 5.2.1 'Sorting by Insertion', that [the value] ""settles to its proper level"""	"Bubble sort"
14235793	"and that this method of sorting has sometimes been called the ""sifting"" or ""sinking"" technique. This debate is perpetuated by the ease with which one may consider this algorithm from two different but equally valid perspectives: Google CEO Eric Schmidt asked president Barack Obama once during an interview about the best way to sort one million integers – and Obama, pausing for a moment, then replied: ""I think the bubble sort would be the wrong way to go."" Bubble sort Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list, compares"	"Bubble sort"
351729	"Radix sort In computer science, radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. A positional notation is required, but because integers can represent strings of characters (e.g., names or dates) and specially formatted floating point numbers, radix sort is not limited to integers. Radix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines. Most digital computers internally represent all of their data as electronic representations of binary numbers, so processing the digits"	"Radix sort"
351730	"of integer representations by groups of binary digit representations is most convenient. Radix sorts can be implemented to start at either the most significant digit (MSD) or least significant digit (LSD). For example, when sorting the number 1234 into a list, one could start with the 1 or the 4. LSD radix sorts typically use the following sorting order: short keys come before longer keys, and then keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11. MSD"	"Radix sort"
351731	"radix sorts use lexicographic order, which is suitable for sorting strings, such as words, or fixed-length integer representations. A sequence such as ""b, c, d, e, f, g, h, i, j, ba"" would be lexicographically sorted as ""b, ba, c, d, e, f, g, h, i, j"". If lexicographic ordering is used to sort variable-length integer representations, then the representations of the numbers from 1 to 10 would be output as 1, 10, 2, 3, 4, 5, 6, 7, 8, 9, as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter"	"Radix sort"
351732	"keys as long as the longest key for the purpose of determining sorted order. The topic of the efficiency of radix sort compared to other sorting algorithms is somewhat tricky and subject to quite a lot of misunderstandings. Whether radix sort is equally efficient, less efficient or more efficient than the best comparison-based algorithms depends on the details of the assumptions made. Radix sort complexity is for keys which are integers of word size . Sometimes is presented as a constant, which would make radix sort better (for sufficiently large ) than the best comparison-based sorting algorithms, which all perform"	"Radix sort"
351733	"comparisons to sort keys. However, in general cannot be considered a constant: if all keys are distinct, then has to be at least for a random-access machine to be able to store them in memory, which gives at best a time complexity . That would seem to make radix sort at best equally efficient as optimal comparison-based sorts (and worse if keys are much longer than ). The counter argument is that comparison-based algorithms are measured in number of comparisons, not actual time complexity. Under some assumptions the comparisons will be constant time on average, under others they will not."	"Radix sort"
351734	"Comparisons of randomly generated keys takes constant time on average, as keys differ on the very first bit in half the cases, and differ on the second bit in half of the remaining half, and so on, resulting in an average of two bits that need to be compared. In a sorting algorithm the first comparisons made satisfies the randomness condition, but as the sort progresses the keys compared are clearly not randomly chosen anymore. For example, consider a bottom-up merge sort. The first pass will compare pairs of random keys, but the last pass will compare keys that are"	"Radix sort"
351735	"very close in the sorting order. This makes merge sort, on this class of inputs, take time. That assumes all memory accesses cost the same, which is not a physically reasonable assumption as we scale to infinity, and not, in practice, how real computers work. This argument extends from the observation that computers are filled with different types of memory (cache, system memory, virtual memory etc.) in different and limited quantities. Modern operating systems will position variables into these different systems automatically making memory access time differ widely as more and more memory is utilized. A Least significant digit (LSD)"	"Radix sort"
351736	"Radix sort is a fast stable sorting algorithm which can be used to sort keys in integer representation order. Keys may be a string of characters, or numerical digits in a given 'radix'. The processing of the keys begins at the least significant digit (i.e., the rightmost digit), and proceeds to the most significant digit (i.e., the leftmost digit). The sequence in which digits are processed by an LSD radix sort is the opposite of the sequence in which digits are processed by a most significant digit (MSD) radix sort. An LSD radix sort operates in O(""nw"") time, where ""n"""	"Radix sort"
351737	"is the number of keys, and ""w"" is the average key length. This kind of performance for variable-length keys can be achieved by grouping all of the keys that have the same length together and separately performing an LSD radix sort on each group of keys for each length, from shortest to longest, in order to avoid processing the whole list of keys on every sorting pass. A radix sorting algorithm was originally used to sort punched cards in several passes. A computer algorithm was invented for radix sort in 1954 at MIT by Harold H. Seward. In many large"	"Radix sort"
351738	"applications needing speed, the computer radix sort is an improvement on (slower) comparison sorts. LSD radix sorts have resurfaced as an alternative to high performance comparison-based sorting algorithms (like heapsort and merge sort) that require O(""n"" · log ""n"") comparisons, where ""n"" is the number of items to be sorted. Comparison sorts can do no better than O(""n"" · log ""n"") execution time but offer the flexibility of being able to sort with respect to more complicated orderings than a lexicographic one; however, this ability is of little importance in many practical applications. Each key is first figuratively dropped into"	"Radix sort"
351739	"one level of buckets corresponding to the value of the rightmost digit. Each bucket preserves the original order of the keys as the keys are dropped into the bucket. There is a one-to-one correspondence between the buckets and the values that can be represented by a digit plus one bucket for the empty digit, which signifies that the string is exhausted and which is not required if all strings are of same length. Then, the process repeats with the next neighbouring more significant digit until there are no more digits to process. In other words: Original, unsorted list: Interpreting as"	"Radix sort"
351740	"nonnegative decimal numbers with leading zeros possibly suppressed and sorting least significant digit (1's place) gives: Sorting by next digit (10's place) gives: Sorting by most significant digit (100's place) gives: It is important to realize that each of the above steps requires just a single pass over the data, since each item can be placed in its correct bucket without having to be compared with other items. Some radix sort implementations allocate space for buckets by first counting the number of keys that belong in each bucket before moving keys into those buckets. The number of times that each"	"Radix sort"
351741	"digit occurs is stored in an array. Consider the previous list of keys viewed as fixed-length decimals of length 3: The first counting pass starts on the least significant digit of each key, producing an array of bucket sizes: A second counting pass on the next more significant digit of each key will produce an array of bucket sizes: A third and final counting pass on the most significant digit of each key will produce an array of bucket sizes: At least one LSD radix sort implementation now counts the number of times that each digit occurs in each column"	"Radix sort"
351742	"for all columns in a single counting pass. (See the external links section.) Other LSD radix sort implementations allocate space for buckets dynamically as the space is needed. A simple version of an LSD radix sort can be achieved using queues as buckets. The following process is repeated for a number of times equal to the length of the longest key: While this may not be the most efficient radix sort algorithm, it is relatively simple, and still quite efficient. A most significant digit (MSD) radix sort can be used to sort keys in lexicographic order. Unlike a least significant"	"Radix sort"
351743	"digit (LSD) radix sort, a most significant digit radix sort does not necessarily preserve the original order of duplicate keys. An MSD radix sort starts processing the keys from the most significant digit, leftmost digit, to the least significant digit, rightmost digit. This sequence is opposite that of least significant digit (LSD) radix sorts. An MSD radix sort stops rearranging the position of a key when the processing reaches a unique prefix of the key. Some MSD radix sorts use one level of buckets in which to group the keys. See the counting sort and pigeonhole sort articles. Other MSD"	"Radix sort"
351744	"radix sorts use multiple levels of buckets, which form a trie or a path in a trie. A postman's sort / postal sort is a kind of MSD radix sort. A recursively subdividing MSD radix sort algorithm works as follows: Sort the list: <br>170, 045, 075, 090, 002, 024, 802, 066 Sorting by least significant digit (1s place) is not needed, as there is no tens bucket with more than one number. Therefore, the now sorted zero hundreds bucket is concatenated, joined in sequence, with the one hundreds bucket and eight hundreds bucket to give:<br>002, 024, 045, 066, 075, 090,"	"Radix sort"
351745	"170, 802 This example used base ten digits for the sake of readability, but of course binary digits or perhaps bytes might make more sense for a binary computer to process. Binary MSD radix sort, also called binary quicksort, can be implemented in-place by splitting the input array into two bins - the 0s bin and the 1s bin. The 0s bin is grown from the beginning of the array, whereas the 1s bin is grown from the end of the array. The 0s bin boundary is placed before the first array element. The 1s bin boundary is placed after"	"Radix sort"
351746	"the last array element. The most significant bit of the first array element is examined. If this bit is a 1, then the first element is swapped with the element in front of the 1s bin boundary (the last element of the array), and the 1s bin is grown by one element by decrementing the 1s boundary array index. If this bit is a 0, then the first element remains at its current location, and the 0s bin is grown by one element. The next array element examined is the one in front of the 0s bin boundary (i.e. the"	"Radix sort"
351747	"first element that is not in the 0s bin or the 1s bin). This process continues until the 0s bin and the 1s bin reach each other. The 0s bin and the 1s bin are then sorted recursively based on the next bit of each array element. Recursive processing continues until the least significant bit has been used for sorting. Handling signed integers requires treating the most significant bit with the opposite sense, followed by unsigned treatment of the rest of the bits. In-place MSD binary-radix sort can be extended to larger radix and retain in-place capability. Counting sort is"	"Radix sort"
351748	"used to determine the size of each bin and their starting index. Swapping is used to place the current element into its bin, followed by expanding the bin boundary. As the array elements are scanned the bins are skipped over and only elements between bins are processed, until the entire array has been processed and all elements end up in their respective bins. The number of bins is the same as the radix used - e.g. 16 bins for 16-Radix. Each pass is based on a single digit (e.g. 4-bits per digit in the case of 16-Radix), starting from the"	"Radix sort"
351749	"most significant digit. Each bin is then processed recursively using the next digit, until all digits have been used for sorting. Neither in-place binary-radix sort nor n-bit-radix sort, discussed in paragraphs above, are stable algorithms. MSD Radix Sort can be implemented as a stable algorithm, but requires the use of a memory buffer of the same size as the input array. This extra memory allows the input buffer to be scanned from the first array element to last, and move the array elements to the destination bins in the same order. Thus, equal elements will be placed in the memory"	"Radix sort"
351750	"buffer in the same order they were in the input array. The MSD-based algorithm uses the extra memory buffer as the output on the first level of recursion, but swaps the input and output on the next level of recursion, to avoid the overhead of copying the output result back to the input buffer. Each of the bins are recursively processed, as is done for the in-place MSD Radix Sort. After the sort by the last digit has been completed, the output buffer is checked to see if it is the original input array, and if it's not, then a"	"Radix sort"
351751	"single copy is performed. If the digit size is chosen such that the key size divided by the digit size is an even number, the copy at the end is avoided. Radix sort, such as two pass method where counting sort is used during the first pass of each level of recursion, has a large constant overhead. Thus, when the bins get small, other sorting algorithms should be used, such as insertion sort. A good implementation of Insertion sort is fast for small arrays, stable, in-place, and can significantly speed up Radix Sort. Note that this recursive sorting algorithm has"	"Radix sort"
351752	"particular application to parallel computing, as each of the bins can be sorted independently. In this case, each bin is passed to the next available processor. A single processor would be used at the start (the most significant digit). By the second or third digit, all available processors would likely be engaged. Ideally, as each subdivision is fully sorted, fewer and fewer processors would be utilized. In the worst case, all of the keys will be identical or nearly identical to each other, with the result that there will be little to no advantage to using parallel computing to sort"	"Radix sort"
351753	"the keys. In the top level of recursion, opportunity for parallelism is in the counting sort portion of the algorithm. Counting is highly parallel, amenable to the parallel_reduce pattern, and splits the work well across multiple cores until reaching memory bandwidth limit. This portion of the algorithm has data-independent parallelism. Processing each bin in subsequent recursion levels is data-dependent, however. For example, if all keys were of the same value, then there would be only a single bin with any elements in it, and no parallelism would be available. For random inputs all bins would be near equally populated and"	"Radix sort"
351754	"a large amount of parallelism opportunity would be available. Note that there are faster sorting algorithms available, for example optimal complexity O(log(""n"")) are those of the Three Hungarians and Richard Cole and Batcher's bitonic merge sort has an algorithmic complexity of O(log(""n"")), all of which have a lower algorithmic time complexity to radix sort on a CREW-PRAM. The fastest known PRAM sorts were described in 1991 by David Powers with a parallelized quicksort that can operate in O(log(n)) time on a CRCW-PRAM with ""n"" processors by performing partitioning implicitly, as well as a radixsort that operates using the same trick"	"Radix sort"
351755	"in O(""k""), where ""k"" is the maximum keylength. However, neither the PRAM architecture or a single sequential processor can actually be built in a way that will scale without the number of constant fan-out gate delays per cycle increasing as O(log(""n"")), so that in effect a pipelined version of Batcher's bitonic mergesort and the O(log(""n"")) PRAM sorts are all O(log(""n"")) in terms of clock cycles, with Powers acknowledging that Batcher's would have lower constant in terms of gate delays than his Parallel quicksort and radix sort, or Cole's merge sort, for a keylength-independent sorting network of O(nlog(""n"")). Another way to"	"Radix sort"
351756	"proceed with an MSD radix sort is to use more memory to create a trie to represent the keys and then traverse the trie to visit each key in order. A depth-first traversal of a trie starting from the root node will visit each key in order. A depth-first traversal of a trie, or any other kind of acyclic tree structure, is equivalent to traversing a maze via the right-hand rule. A trie essentially represents a set of strings or numbers, and a radix sort which uses a trie structure is not necessarily stable, which means that the original order"	"Radix sort"
351757	"of duplicate keys is not necessarily preserved, because a set does not contain duplicate elements. Additional information will have to be associated with each key to indicate the population count or original order of any duplicate keys in a trie-based radix sort if keeping track of that information is important for a particular application. It may even be desirable to discard any duplicate strings as the trie creation proceeds if the goal is to find only unique strings in sorted order. Some people sort a list of strings first and then make a separate pass through the sorted list to"	"Radix sort"
351758	"discard duplicate strings, which can be slower than using a trie to simultaneously sort and discard duplicate strings in one pass. One of the advantages of maintaining the trie structure is that the trie makes it possible to determine quickly if a particular key is a member of the set of keys in a time that is proportional to the length of the key, ""k"", in O(""k"") time, that is ""independent"" of the total number of keys. Determining set membership in a plain list, as opposed to determining set membership in a trie, requires binary search, O(""k log(n)"") time; linear"	"Radix sort"
351759	"search, O(""kn"") time; or some other method whose execution time is in some way dependent on the total number, ""n"", of all of the keys in the worst case. It is sometimes possible to determine set membership in a plain list in O(""k"") time, in a time that is independent of the total number of keys, such as when the list is known to be in an arithmetic sequence or some other computable sequence. Maintaining the trie structure also makes it possible to insert new keys into the set incrementally or delete keys from the set incrementally while maintaining sorted"	"Radix sort"
351760	"order in O(""k"") time, in a time that is independent of the total number of keys. In contrast, other radix sorting algorithms must, in the worst case, re-sort the entire list of keys each time that a new key is added or deleted from an existing list, requiring O(""kn"") time. If the nodes were rooms connected by hallways, then here is how Snow White might proceed to visit all of the dwarfs if the place were dark, keeping her right hand on a wall at all times: These series of steps serve to illustrate the path taken in the trie"	"Radix sort"
351761	"by Snow White via a depth-first traversal to visit the dwarfs by the ascending order of their names, Bashful, Doc, Dopey, Grumpy, Happy, Sleepy, and Sneezy. The algorithm for performing some operation on the data associated with each node of a tree first, such as printing the data, and then moving deeper into the tree is called a pre-order traversal, which is a kind of depth-first traversal. A pre-order traversal is used to process the contents of a trie in ascending order. If Snow White wanted to visit the dwarfs by the descending order of their names, then she could"	"Radix sort"
351762	"walk backwards while following the wall with her right hand, or, alternatively, walk forward while following the wall with her left hand. The algorithm for moving deeper into a tree first until no further descent to unvisited nodes is possible and then performing some operation on the data associated with each node is called post-order traversal, which is another kind of depth-first traversal. A post-order traversal is used to process the contents of a trie in descending order. The root node of the trie in the diagram essentially represents a null string, an empty string, which can be useful for"	"Radix sort"
351763	"keeping track of the number of blank lines in a list of words. The null string can be associated with a circularly linked list with the null string initially as its only member, with the forward and backward pointers both initially pointing to the null string. The circularly linked list can then be expanded as each new key is inserted into the trie. The circularly linked list is represented in the following diagram as thick, grey, horizontally linked lines: If a new key, other than the null string, is inserted into a leaf node of the trie, then the computer"	"Radix sort"
351764	"can go to the last preceding node where there was a key or a bifurcation to perform a depth-first search to find the lexicographic successor or predecessor of the inserted key for the purpose of splicing the new key into the circularly linked list. The last preceding node where there was a key or a bifurcation, a fork in the path, is a parent node in the type of trie shown here, where only unique string prefixes are represented as paths in the trie. If there is already a key associated with the parent node that would have been visited"	"Radix sort"
351765	"during a movement ""away"" from the root during a right-hand, forward-moving, depth-first traversal, then that immediately ends the depth-first search, as that key is the predecessor of the inserted key. For example, if Bashful is inserted into the trie, then the predecessor is the null string in the parent node, which is the root node in this case. In other words, if the key that is being inserted is on the leftmost branch of the parent node, then any string contained in the parent node is the lexicographic predecessor of the key that is being inserted, else the lexicographic predecessor"	"Radix sort"
351766	"of the key that is being inserted exists down the parent node's branch that is immediately to the left of the branch where the new key is being inserted. For example, if Grumpy were the last key inserted into the trie, then the computer would have a choice of trying to find either the predecessor, Dopey, or the successor, Happy, with a depth-first search starting from the parent node of Grumpy. With no additional information to indicate which path is longer, the computer might traverse the longer path, D, O, P. If Dopey were the last key inserted into the"	"Radix sort"
351767	"trie, then the depth-first search starting from the parent node of Dopey would soon find the predecessor, ""Doc"", because that would be the only choice. If a new key is inserted into an internal node, then a depth-first search can be started from the internal node to find the lexicographic successor. For example, if the literal string ""DO"" were inserted in the node at the end of the path D, O, then a depth-first search could be started from that internal node to find the successor, ""DOC"", for the purpose of splicing the new string into the circularly linked list."	"Radix sort"
351768	"Forming the circularly linked list requires more memory but allows the keys to be visited more directly in either ascending or descending order via a linear traversal of the linked list rather than a depth-first traversal of the entire trie. This concept of a circularly linked trie structure is similar to the concept of a threaded binary tree. This structure will be called a circularly threaded trie. When a trie is used to sort numbers, the number representations must all be the same length unless you are willing to perform a breadth-first traversal. When the number representations will be visited"	"Radix sort"
351769	"via depth-first traversal, as in the above diagram, the number representations will always be on the leaf nodes of the trie. Note how similar in concept this particular example of a trie is to the recursive forward radix sort example which involves the use of buckets instead of a trie. Performing a radix sort with the buckets is like creating a trie and then discarding the non-leaf nodes. Radix sort In computer science, radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position"	"Radix sort"
1390077	"Cocktail shaker sort Cocktail shaker sort, also known as bidirectional bubble sort, cocktail sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is a variation of bubble sort that is both a stable sorting algorithm and a comparison sort. The algorithm differs from a bubble sort in that it sorts in both directions on each pass through the list. This sorting algorithm is only marginally more difficult to implement than a bubble sort, and solves the problem of turtles in bubble sorts. It provides only marginal performance improvements, and"	"Cocktail shaker sort"
1390078	"does not improve asymptotic performance; like the bubble sort, it is not of practical interest (insertion sort is preferred for simple sorts), though it finds some use in education. The simplest form goes through the whole list each time: The first rightward pass will shift the largest element to its correct place at the end, and the following leftward pass will shift the smallest element to its correct place at the beginning. The second complete pass will shift the second largest and second smallest elements to their correct places, and so on. After ""i"" passes, the first ""i"" and the"	"Cocktail shaker sort"
1390079	"last ""i"" elements in the list are in their correct positions, and do not need to be checked. By shortening the part of the list that is sorted each time, the number of operations can be halved (see bubble sort). This is an example of the algorithm in MATLAB/OCTAVE with the optimization of remembering the last swap index and updating the bounds. Cocktail shaker sort is a slight variation of bubble sort. It differs in that instead of repeatedly passing through the list from bottom to top, it passes alternately from bottom to top and then from top to bottom."	"Cocktail shaker sort"
1390080	"It can achieve slightly better performance than a standard bubble sort. The reason for this is that bubble sort only passes through the list in one direction and therefore can only move items backward one step each iteration. An example of a list that proves this point is the list (2,3,4,5,1), which would only need to go through one pass of cocktail sort to become sorted, but if using an ascending bubble sort would take four passes. However one cocktail sort pass should be counted as two bubble sort passes. Typically cocktail sort is less than two times faster than"	"Cocktail shaker sort"
1390081	"bubble sort. Another optimization can be that the algorithm remembers where the last actual swap has been done. In the next iteration, there will be no swaps beyond this limit and the algorithm has shorter passes. As the cocktail shaker sort goes bidirectionally, the range of possible swaps, which is the range to be tested, will reduce per pass, thus reducing the overall running time slightly. The complexity of the cocktail shaker sort in big O notation is formula_1 for both the worst case and the average case, but it becomes closer to formula_2 if the list is mostly ordered"	"Cocktail shaker sort"
1390082	"before applying the sorting algorithm. For example, if every element is at a position that differs by at most k (k ≥ 1) from the position it is going to end up in, the complexity of cocktail shaker sort becomes formula_3 The cocktail shaker sort is also briefly discussed in the book ""The Art of Computer Programming"", along with similar refinements of bubble sort. In conclusion, Knuth states about bubble sort and its improvements: Cocktail shaker sort Cocktail shaker sort, also known as bidirectional bubble sort, cocktail sort, shaker sort (which can also refer to a variant of selection sort),"	"Cocktail shaker sort"
14962228	"Cycle sort Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result. Unlike nearly every other sort, items are ""never"" written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position,"	"Cycle sort"
14962229	"or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort. Minimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory. To illustrate the idea of cycle sort, consider a list with distinct elements. Given an element , we can find the index at which it will occur in the ""sorted list"" by simply counting the number of elements in the entire list that are smaller"	"Cycle sort"
14962230	"than . Now Repeating this process for every element sorts the list, with a single writing operation if and only if an element is not already at its correct position. While computing the correct positions takes formula_1 time for every single element, thus resulting in a quadratic time algorithm, the number of writing operations is minimized. To create a working implementation from the above outline, two issues need to be addressed: The following Python implementation performs cycle sort on an array, counting the number of writes to that array that were needed to sort it. When the array contains only"	"Cycle sort"
14962231	"duplicates of a relatively small number of items, a constant-time perfect hash function can greatly speed up finding where to put an item, turning the sort from Θ(""n"") time to Θ(""n"" + ""k"") time, where ""k"" is the total number of hashes. The array ends up sorted in the order of the hashes, so choosing a hash function that gives you the right ordering is important. Before the sort, create a histogram, sorted by hash, counting the number of occurrences of each hash in the array. Then create a table with the cumulative sum of each entry in the histogram."	"Cycle sort"
14962232	"The cumulative sum table will then contain the position in the array of each element. The proper place of elements can then be found by a constant-time hashing and cumulative sum table lookup rather than a linear search. ""Cycle-Sort: A Linear Sorting Method"", The Computer Journal (1990) 33 (4): 365-367. Cycle sort Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored"	"Cycle sort"
333348	"Pigeonhole sort Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (""n"") and the length of the range of possible key values (""N"") are approximately the same. It requires O(""n"" + ""N"") time. It is similar to counting sort, but differs in that it ""moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array then uses the array to compute each item's final destination and move the item there."" The pigeonhole algorithm works as follows: Suppose one were sorting"	"Pigeonhole sort"
333349	"these value pairs by their first element: For each value between 3 and 8 we set up a pigeonhole, then move each element to its pigeonhole: The pigeonhole array is then iterated over in order, and the elements are moved back to the original list. The difference between pigeonhole sort and counting sort is that in counting sort, the auxiliary array does not contain lists of input elements, only counts: Using this information, one could perform a series of exchanges on the input array that would put it in order, moving items only once. For arrays where ""N"" is much"	"Pigeonhole sort"
333350	"larger than ""n"", bucket sort is a generalization that is more efficient in space and time. Pigeonhole sort Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (""n"") and the length of the range of possible key values (""N"") are approximately the same. It requires O(""n"" + ""N"") time. It is similar to counting sort, but differs in that it ""moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array then uses the array to compute each item's final"	"Pigeonhole sort"
966058	"Bogosort In computer science, bogosort (also known as permutation sort, stupid sort, slowsort, shotgun sort or monkey sort) is a highly ineffective sorting function based on the generate and test paradigm. The function successively generates permutations of its input until it finds one that is sorted. It is not useful for sorting, but may be used for educational purposes, to contrast it with more efficient algorithms. Two versions of the function exist: a deterministic version that enumerates all permutations until it hits a sorted one, and a randomized version that randomly permutes its input. An analogy for the working of"	Bogosort
966059	"the latter version is to sort a deck of cards by throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name is a portmanteau the words ""bogus"" and ""sort"". The following is a description of the randomized function in pseudocode: Here is the above pseudocode re-written in Python 3: import random def is_sorted(data): def bogosort(data): This code assumes that data is a simple, mutable datatype—like Python's built-in —whose elements can be compared without issue. Here is an example with shuffle in Standard ML: If all elements to"	Bogosort
966060	"be sorted are distinct, the expected number of comparisons performed in the average case by randomized bogosort is asymptotically equivalent to formula_1, and the expected number of swaps in the average case equals formula_2. The expected number of swaps grows faster than the expected number of comparisons, because if the elements are not in order, this will usually be discovered after only a few comparisons, no matter how many elements there are; but the work of shuffling the collection is proportional to its size. In the worst case, the number of comparisons and swaps are both unbounded, for the same"	Bogosort
966061	"reason that a tossed coin might turn up heads any number of times in a row. The best case occurs if the list as given is already sorted; in this case the expected number of comparisons is formula_3, and no swaps at all are carried out. For any collection of fixed size, the expected running time of the algorithm is finite for much the same reason that the infinite monkey theorem holds: there is some probability of getting the right permutation, so given an unbounded number of tries it will almost surely eventually be chosen. Here is some Python code"	Bogosort
966062	"to efficiently test the average complexity of Bogosort. import sys import time import random from multiprocessing import Process, Queue import numpy as np import matplotlib.pyplot as plt from scipy.special import factorial from tqdm import tqdm WORKCOUNT = 8 TRIALCOUNT = 10 def main(): def is_sorted(some_list): class Sorter(Process): if __name__ == '__main__': Bogosort In computer science, bogosort (also known as permutation sort, stupid sort, slowsort, shotgun sort or monkey sort) is a highly ineffective sorting function based on the generate and test paradigm. The function successively generates permutations of its input until it finds one that is sorted. It is not"	Bogosort
401042	"Selection sort In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort. It has O(""n"") time complexity, making it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited. The algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted"	"Selection sort"
401043	"that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right. Here is an example of this sort algorithm sorting five elements: Selection sort can also be used on list structures that make add and remove efficient, such as a linked list. In this case it is more"	"Selection sort"
401044	"common to ""remove"" the minimum element from the remainder of the list, and then ""insert"" it at the end of the values sorted so far. For example: Selection sort is not difficult to analyze compared to other sorting algorithms since none of the loops depend on the data in the array. Selecting the minimum requires scanning formula_1 elements (taking formula_2 comparisons) and then swapping it into the first position. Finding the next lowest element requires scanning the remaining formula_2 elements and so on. Therefore, the total number of comparisons is formula_4 By the hockey-stick identity, formula_5 which is of complexity"	"Selection sort"
401045	"formula_6 in terms of number of comparisons. Each of these scans requires one swap for formula_2 elements (the final element is already in place). Among simple average-case Θ(""n"") algorithms, selection sort almost always outperforms bubble sort and gnome sort. Insertion sort is very similar in that after the ""k""th iteration, the first ""k"" elements in the array are in sorted order. Insertion sort's advantage is that it only scans as many elements as it needs in order to place the ""k"" + 1st element, while selection sort must scan all remaining elements to find the ""k"" + 1st element. Simple"	"Selection sort"
401046	"calculation shows that insertion sort will therefore usually perform about half as many comparisons as selection sort, although it can perform just as many or far fewer depending on the order the array was in prior to sorting. It can be seen as an advantage for some real-time applications that selection sort will perform identically regardless of the order of the array, while insertion sort's running time can vary considerably. However, this is more often an advantage for insertion sort in that it runs much more efficiently if the array is already sorted or ""close to sorted."" While selection sort"	"Selection sort"
401047	"is preferable to insertion sort in terms of number of writes (Θ(""n"") swaps versus Ο(""n"") swaps), it almost always far exceeds (and never beats) the number of writes that cycle sort makes, as cycle sort is theoretically optimal in the number of writes. This can be important if writes are significantly more expensive than reads, such as with EEPROM or Flash memory, where every write lessens the lifespan of the memory. Finally, selection sort is greatly outperformed on larger arrays by Θ(""n"" log ""n"") divide-and-conquer algorithms such as mergesort. However, insertion sort or selection sort are both typically faster for"	"Selection sort"
401048	"small arrays (i.e. fewer than 10–20 elements). A useful optimization in practice for the recursive algorithms is to switch to insertion sort or selection sort for ""small enough"" sublists. Heapsort greatly improves the basic algorithm by using an implicit heap data structure to speed up finding and removing the lowest datum. If implemented correctly, the heap will allow finding the next lowest element in Θ(log ""n"") time instead of Θ(""n"") for the inner loop in normal selection sort, reducing the total running time to Θ(""n"" log ""n""). A bidirectional variant of selection sort, called cocktail sort, is an algorithm which"	"Selection sort"
401049	"finds both the minimum and maximum values in the list in every pass. This reduces the number of scans of the list by a factor of 2, eliminating some loop overhead but not actually decreasing the number of comparisons or swaps. Note, however, that cocktail sort more often refers to a bidirectional variant of bubble sort. Sometimes this is double selection sort. Selection sort can be implemented as a stable sort. If, rather than swapping in step 2, the minimum value is inserted into the first position (that is, all intervening items moved down), the algorithm is stable. However, this"	"Selection sort"
401050	"modification either requires a data structure that supports efficient insertions or deletions, such as a linked list, or it leads to performing Θ(""n"") writes. In the bingo sort variant, items are ordered by repeatedly looking through the remaining items to find the greatest value and moving all items with that value to their final location. Like counting sort, this is an efficient variant if there are many duplicate values. Indeed, selection sort does one pass through the remaining items for each item moved. Bingo sort does one pass for each value (not item): after an initial pass to find the"	"Selection sort"
401051	"biggest value, the next passes can move every item with that value to its final location while finding the next value as in the following pseudocode (arrays are zero-based and the for-loop includes both the top and bottom limits, as in Pascal): Thus, if on average there are more than two items with the same value, bingo sort can be expected to be faster because it executes the inner loop fewer times than selection sort. Selection sort In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort. It has O(""n"") time complexity, making it inefficient on"	"Selection sort"
965949	"Counting sort In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum and minimum key values, so it is only suitable for direct use in situations where the variation in keys is not"	"Counting sort"
965950	"significantly greater than the number of items. However, it is often used as a subroutine in another sorting algorithm, radix sort, that can handle larger keys more efficiently. Because counting sort uses key values as indexes into an array, it is not a comparison sort, and the Ω(""n"" log ""n"") lower bound for comparison sorting does not apply to it. Bucket sort may be used for many of the same tasks as counting sort, with a similar time analysis; however, compared to counting sort, bucket sort requires linked lists, dynamic arrays or a large amount of preallocated memory to hold"	"Counting sort"
965951	"the sets of items within each bucket, whereas counting sort instead stores a single number (the count of items) per bucket. In the most general case, the input to counting sort consists of a collection of items, each of which has a non-negative integer key whose maximum value is at most . In some descriptions of counting sort, the input to be sorted is assumed to be more simply a sequence of integers itself, but this simplification does not accommodate many applications of counting sort. For instance, when used as a subroutine in radix sort, the keys for each call"	"Counting sort"
965952	"to counting sort are individual digits of larger item keys; it would not suffice to return only a sorted list of the key digits, separated from the items. In applications such as in radix sort, a bound on the maximum key value will be known in advance, and can be assumed to be part of the input to the algorithm. However, if the value of is not already known then it may be computed, as a first step, by an additional loop over the data to determine the maximum key value that actually occurs within the data. The output is"	"Counting sort"
965953	"an array of the items, in order by their keys. Because of the application to radix sorting, it is important for counting sort to be a stable sort: if two items have the same key as each other, they should have the same relative position in the output as they did in the input. In summary, the algorithm loops over the items, computing a histogram of the number of times each key occurs within the input collection. It then performs a prefix sum computation (a second loop, over the range of possible keys) to determine, for each key, the starting"	"Counting sort"
965954	"position in the output array of the items having that key. Finally, it loops over the items again, moving each item into its sorted position in the output array. In pseudocode, this may be expressed as follows: After the first for loop, codice_1 stores the number of items with key equal to codice_2. After the second for loop, it instead stores the number of items with key less than codice_2, which is the same as the first index at which an item with key codice_2 should be stored in the output array. Throughout the third loop, codice_1 always stores the"	"Counting sort"
965955	"next position in the output array into which an item with key codice_2 should be stored, so each item is moved into its correct position in the output array. The relative order of items with equal keys is preserved here; i.e., this is a stable sort. Because the algorithm uses only simple for loops, without recursion or subroutine calls, it is straightforward to analyze. The initialization of the count array, and the second for loop which performs a prefix sum on the count array, each iterate at most times and therefore take time. The other two for loops, and the"	"Counting sort"
965956	"initialization of the output array, each take time. Therefore, the time for the whole algorithm is the sum of the times for these steps, . Because it uses arrays of length and , the total space usage of the algorithm is also . For problem instances in which the maximum key value is significantly smaller than the number of items, counting sort can be highly space-efficient, as the only storage it uses other than its input and output arrays is the Count array which uses space . If each item to be sorted is itself an integer, and used as"	"Counting sort"
965957	"key as well, then the second and third loops of counting sort can be combined; in the second loop, instead of computing the position where items with key codice_2 should be placed in the output, simply append codice_8 copies of the number codice_2 to the output. This algorithm may also be used to eliminate duplicate keys, by replacing the codice_10 array with a bit vector that stores a codice_11 for a key that is present in the input and a codice_12 for a key that is not present. If additionally the items are the integer keys themselves, both second and"	"Counting sort"
965958	"third loops can be omitted entirely and the bit vector will itself serve as output, representing the values as offsets of the non-codice_12 entries, added to the range's lowest value. Thus the keys are sorted and the duplicates are eliminated in this variant just by being placed into the bit array. For data in which the maximum key size is significantly smaller than the number of data items, counting sort may be parallelized by splitting the input into subarrays of approximately equal size, processing each subarray in parallel to generate a separate count array for each subarray, and then merging"	"Counting sort"
965959	"the count arrays. When used as part of a parallel radix sort algorithm, the key size (base of the radix representation) should be chosen to match the size of the split subarrays. The simplicity of the counting sort algorithm and its use of the easily parallelizable prefix sum primitive also make it usable in more fine-grained parallel algorithms. As described, counting sort is not an in-place algorithm; even disregarding the count array, it needs separate input and output arrays. It is possible to modify the algorithm so that it places the items into sorted order within the same array that"	"Counting sort"
965960	"was given to it as the input, using only the count array as auxiliary storage; however, the modified in-place version of counting sort is not stable. Although radix sorting itself dates back far longer, counting sort, and its application to radix sorting, were both invented by Harold H. Seward in 1954. Counting sort In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on"	"Counting sort"
13683257	"Timsort Timsort is a hybrid sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It uses techniques from Peter McIlroy's ""Optimistic Sorting and Information Theoretic Complexity"", in ""Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms"", pp. 467–474, January 1993. It was implemented by Tim Peters in 2002 for use in the Python programming language. The algorithm finds subsequences of the data that are already ordered, and uses that knowledge to sort the remainder more efficiently. This is done by merging an identified subsequence, called a run, with existing"	Timsort
13683258	"runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, and in GNU Octave. Timsort was designed to take advantage of ""runs"" of consecutive ordered elements that already exist in most real-world data, ""natural runs"". It iterates over the data collecting elements into runs, and simultaneously merging those runs together. When there are runs, doing this decreases the total number of comparisons needed to fully sort the list. Timsort iterates over the data looking for ""natural"	Timsort
13683259	"runs"" of at least two elements that are either non-descending (each element is greater than or equal to its predecessor) or strictly descending (each element is less than its predecessor). Since descending runs are later blindly reversed, excluding equal elements maintains the algorithm's stability; i.e., equal elements won't be reversed. Note that any two elements are guaranteed to be either descending or non-descending. A reference to each run is then pushed onto a stack. Timsort is an adaptive sort, using insertion sort to combine runs smaller than the ""minimum run size"" (""minrun""), and merge sort otherwise. ""Minrun"" is selected so"	Timsort
13683260	"most runs in a random array are, or become, ""minrun"" in length. It also results in a reasonable number of function calls in the implementation of the sort. Because merging is most efficient when the number of runs is equal to, or slightly less than, a power of two, and notably less efficient when the number of runs is slightly more than a power of two, Timsort chooses ""minrun"" to try to ensure the former condition. ""Minrun"" is chosen from the range 32 to 64 inclusive, such that the size of the data, divided by ""minrun"", is equal to, or"	Timsort
13683261	"slightly less than, a power of two. The final algorithm takes the six most significant bits of the size of the array, adds one if any of the remaining bits are set, and uses that result as the ""minrun"". This algorithm works for all arrays, including those smaller than 64; for arrays of size 63 or less, this sets ""minrun"" equal to the array size and Timsort reduces to an insertion sort. Concurrently with the search for runs, the runs are merged with mergesort. Except where Timsort tries to optimise for merging disjoint runs in galloping mode, runs are repeatedly"	Timsort
13683262	"merged two at a time, with the only concerns being to maintain stability and merge balance. Stability requires non-consecutive runs are not merged, as elements could be transferred across equal elements in the intervening run, violating stability. Further, it would be impossible to recover the order of the equal elements at a later point. In pursuit of balanced merges, Timsort considers three runs on the top of the stack, ""X"", ""Y"", ""Z"", and maintains the invariants: If the invariants are violated, ""Y"" is merged with the smaller of ""X"" or ""Z"" and the invariants are checked again. Once the invariants"	Timsort
13683263	"hold, the next run is formed. Somewhat inappreciably, the invariants maintain merges as being approximately balanced while maintaining a compromise between delaying merging for balance, and exploiting fresh occurrence of runs in cache memory, and also making merge decisions relatively simple. On reaching the end of the data, Timsort repeatedly merges the two runs on the top of the stack, until only one run of the entire data remains. Timsort performs an almost in-place merge sort, as actual in-place merge sort implementations have a high overhead. First Timsort performs a binary search to find the location in the first run"	Timsort
13683264	"of the first element in the second run, and the location in the second run of the last element in the first run. Elements before and after these locations are already in their correct place, and may be removed from further consideration. This not only optimises element movements and running time, but also allows the use of less temporary memory. Then the smaller of the remaining runs is copied into temporary memory, and elements are merged with the larger run, into the now free space. If the first run is smaller, the merge starts at the beginning; if the second"	Timsort
13683265	"is smaller, the merge starts at the end. Say, for example, two runs ""A"" and ""B"" are to be merged, with ""A"" being the smaller run. In this case a binary search examines ""A"" to find the first element ""a""ʹ larger than the first element of ""B"". Note that ""A"" and ""B"" are already sorted individually. When ""a""ʹ is found, the algorithm can ignore elements before that position when merging. Similarly, the algorithm also looks for the first element ""b""ʹ in ""B"" greater than the last element of ""A"". The elements after ""b""ʹ can also be ignored when merging. This"	Timsort
13683266	"preliminary searching is not efficient for highly random data, but is efficient in other situations and is hence included. An individual merge keeps a count of consecutive elements selected from the same input set. The algorithm switches to galloping mode when this reaches the ""minimum galloping threshold"" (""min_gallop"") in an attempt to capitalise on sub-runs in the data. The success or failure of galloping is used to adjust ""min_gallop"", as an indication of whether the data does or does not contain sufficient sub-runs. In galloping mode, the algorithm searches for the first element of one array in the other. This"	Timsort
13683267	"is done by comparing that initial element with the (2 − 1)th element of the other array (first, third, seventh, and so on) so as to get a range of elements between which the initial element will lie. This shortens the range for binary searching, thus increasing efficiency. In cases where galloping is found to be less efficient than binary search, galloping mode is exited. Galloping is beneficial only when the initial element of one run is not one of the first seven elements of the other run. This implies an initial threshold of 7. To avoid the drawbacks of"	Timsort
13683268	"galloping mode, the merging functions adjust the threshold value. If the selected element is from the same array that returned an element previously, ""min_gallop"" is reduced by one. Otherwise, the value is incremented by one, thus discouraging a return to galloping mode. In the case of random data, the value of ""min_gallop"" becomes so large that galloping mode never recurs. When merging is done right-to-left, galloping starts from the right end of the data, that is, the last element. Galloping from the beginning also gives the required results, but makes more comparisons. Thus, the galloping algorithm uses a variable that"	Timsort
13683269	"gives the index at which galloping should begin. Timsort can enter galloping mode at any index and continue checking at the next index which is offset by 1, 3, 7, …, (2 − 1)… and so on from the current index. In the case of right-to-left merging, the offsets to the index will be −1, −3, −7, … Galloping is not always efficient. In some cases galloping mode requires more comparisons than a simple linear search. While for the first few cases both modes may require the same number of comparisons, over time galloping mode requires 33% more comparisons than"	Timsort
13683270	"linear search to arrive at the same results. In the worst case, Timsort takes formula_1 comparisons to sort an array of elements. In the best case, which occurs when the input is already sorted, it runs in linear time, meaning that it is an adaptive sorting algorithm. In 2015, Dutch and German researchers in the EU FP7 ENVISAGE project found a bug in the standard implementation of Timsort. Specifically, the invariants on stacked run sizes ensure a tight upper bound on the maximum size of the required stack. The implementation preallocated a stack sufficient to sort 2 bytes of input,"	Timsort
13683271	"and avoided further overflow checks. However, the guarantee requires the invariants to apply to ""every"" group of three consecutive runs, but the implementation only checked it for the top three. Using the KeY tool for formal verification of Java software, the researchers found that this check is not sufficient, and they were able to find run lengths (and inputs which generated those run lengths) which would result in the invariants being violated deeper in the stack after the top of the stack was merged. As a consequence, for certain inputs the allocated size is not sufficient to hold all unmerged"	Timsort
13683272	"runs. In Java, this generates for those inputs an array-out-of-bound exception. The smallest input that triggers this exception in Java and Android v7 is of size . (Older Android versions already triggered this exception for certain inputs of size ) The Java implementation was corrected by increasing the size of the preallocated stack based on an updated worst-case analysis. The article also showed by formal methods how to establish the intended invariant by checking that the ""four"" topmost runs in the stack satisfy the two rules above. This approach was adopted by Python and Android. Timsort Timsort is a hybrid"	Timsort
8537183	"Tree sort A tree sort is a sort algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree (in-order) so that the elements come out in sorted order. Its typical use is sorting elements online: after each insertion, the set of elements seen so far is available in sorted order. Adding one item to a binary search tree is on average an process (in big O notation). Adding n items is an process, making tree sorting a 'fast sort' process. Adding an item to an unbalanced binary tree requires time in the"	"Tree sort"
8537184	"worst-case: When the tree resembles a linked list (degenerate tree). This results in a worst case of time for this sorting algorithm. This worst case occurs when the algorithm operates on an already sorted set, or one that is nearly sorted, reversed or nearly reversed. Expected time can however be achieved by shuffling the array, but this does not help for equal items. The worst-case behaviour can be improved by using a self-balancing binary search tree. Using such a tree, the algorithm has an worst-case performance, thus being degree-optimal for a comparison sort. However, trees require memory to be allocated"	"Tree sort"
8537185	"on the heap, which is a significant performance hit when compared to quicksort and heapsort. When using a splay tree as the binary search tree, the resulting algorithm (called splaysort) has the additional property that it is an adaptive sort, meaning that its running time is faster than for inputs that are nearly sorted. The following tree sort algorithm in pseudocode accepts a collection of comparable items and outputs the items in ascending order: In a simple functional programming form, the algorithm (in Haskell) would look something like this: In the above implementation, both the insertion algorithm and the retrieval"	"Tree sort"
8537186	"algorithm have worst-case scenarios. Tree sort A tree sort is a sort algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree (in-order) so that the elements come out in sorted order. Its typical use is sorting elements online: after each insertion, the set of elements seen so far is available in sorted order. Adding one item to a binary search tree is on average an process (in big O notation). Adding n items is an process, making tree sorting a 'fast sort' process. Adding an item to an unbalanced binary tree"	"Tree sort"
2901611	"Stooge sort Stooge sort is a recursive sorting algorithm. It is notable for its exceptional bad time complexity of . The running time of the algorithm is thus slower compared to reasonable sorting algorithms, and is slower than Bubble sort, a canonical example of a fairly inefficient sort. It is however more efficient than Slowsort. The name comes from The Three Stooges. The algorithm is defined as follows: It is important to get the integer sort size used in the recursive calls by rounding the 2/3 ""upwards"", e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise"	"Stooge sort"
2901612	"the sort can fail on certain data. However, if the code is written to end on a base case of size 1, rather than terminating on either size 1 or size 2, rounding the 2/3 of 2 upwards gives an infinite number of calls. Stooge sort Stooge sort is a recursive sorting algorithm. It is notable for its exceptional bad time complexity of . The running time of the algorithm is thus slower compared to reasonable sorting algorithms, and is slower than Bubble sort, a canonical example of a fairly inefficient sort. It is however more efficient than Slowsort. The"	"Stooge sort"
17934392	"Block sort Block sort, or block merge sort, is a sorting algorithm combining at least two merge operations with an insertion sort to arrive at in-place stable sorting. It gets its name from the observation that merging two sorted lists, and , is equivalent to breaking into evenly sized ""blocks"", inserting each block into under special rules, and merging pairs. One practical algorithm for O(log n) in place merging was proposed by Pok-Son Kim and Arne Kutzner in 2008. The outer loop of block sort is identical to a bottom-up merge sort, where each ""level"" of the sort merges pairs"	"Block sort"
17934393	"of subarrays, and , in sizes of 1, then 2, then 4, 8, 16, and so on, until both subarrays combined are the array itself. Rather than merging and directly as with traditional methods, a block-based merge algorithm divides into discrete blocks of size (resulting in ""number"" of blocks as well), inserts each block into such that the first value of each block is less than or equal (≤) to the value immediately after it, then ""locally merges"" each block with any values between it and the next block. As merges still require a separate buffer large enough to hold"	"Block sort"
17934394	"the block to be merged, two areas within the array are reserved for this purpose (known as ""internal buffers""). The first two blocks are thus modified to contain the first instance of each value within , with the original contents of those blocks shifted over if necessary. The remaining blocks are then inserted into and merged using one of the two buffers as swap space. This process causes the values in that buffer to be rearranged. Once every and block of every and subarray have been merged for that level of the merge sort, the values in that buffer must"	"Block sort"
17934395	"be sorted to restore their original order, so an insertion sort must be applied. The values in the buffers are then redistributed to their first sorted position within the array. This process repeats for each level of the outer bottom-up merge sort, at which point the array will have been stably sorted. The following operators are used in the code examples: Additionally, block sort relies on the following operations as part of its overall algorithm: As previously stated, the outer loop of a block sort is identical to a bottom-up merge sort. However, it benefits from the variant that ensures"	"Block sort"
17934396	"each A and B subarray are the same size to within one item: Fixed-point math may also be used, by representing the scale factor as a fraction codice_1: The two internal buffers needed for each level of the merge step are created by moving the first 2 instances of each value within an A subarray to the start of A. First it iterates over the elements in A and counts off the unique values it needs, then it applies array rotations to move those unique values to the start. If A did not contain enough unique values to fill the"	"Block sort"
17934397	"two buffers (of size each), B can be used just as well. In this case it moves the ""last"" instance of each value to the ""end"" of B, with that part of B not being included during the merges. If B does not contain enough unique values either, it pulls out the largest number of unique values it ""could"" find, then adjusts the size of the A and B blocks such that the number of resulting A blocks is less than or equal to the number of unique items pulled out for the buffer. Only one buffer will be used"	"Block sort"
17934398	"in this case – the second buffer won't exist. Once the one or two internal buffers have been created, it begins merging each A and B subarray for this level of the merge sort. To do so, it divides each A and B subarray into evenly sized blocks of the size calculated in the previous step, where the first A block and last B block are unevenly sized if needed. It then loops over each of the evenly sized A blocks and swaps the second value with a corresponding value from the first of the two internal buffers. This is"	"Block sort"
17934399	"known as ""tagging"" the blocks. After defining and tagging the A blocks in this manner, the A blocks are ""rolled"" through the B blocks by block swapping the first evenly sized A block with the next B block. This process repeats until the first value of the A block with the smallest tag value is less than or equal to the last value of the B block that was just swapped with an A block. At that point, the minimum A block (the A block with the smallest tag value) is swapped to the start of the rolling A blocks"	"Block sort"
17934400	"and the tagged value is restored with its original value from the first buffer. This is known as ""dropping"" a block behind, as it will no longer be rolled along with the remaining A blocks. That A block is then inserted into the previous B block, first by using a binary search on B to find the index where the first value of A is less than or equal to the value at that index of B, and then by rotating A into B at that index. One optimization that can be applied during this step is the ""floating-hole technique""."	"Block sort"
17934401	"When the minimum A block is dropped behind and needs to be rotated into the previous B block, after which its contents are swapped into the second internal buffer for the local merges, it would be faster to swap the A block to the buffer beforehand, and to take advantage of the fact that the contents of that buffer do not need to retain any order. So rather than rotating the second buffer (which used to be the A block before the block swap) into the previous B block at position ""index"", the values in the B block after ""index"""	"Block sort"
17934402	"can simply be block swapped with the last items of the buffer. The ""floating hole"" in this case refers to the contents of the second internal buffer ""floating"" around the array, and acting as a ""hole"" in the sense that the items do not need to retain their order. Once the A block has been rotated into the B block, the previous A block is then merged with the B values that follow it, using the second buffer as swap space. When the first A block is dropped behind this refers to the unevenly sized A block at the start,"	"Block sort"
17934403	"when the second A block is dropped behind it means the first A block, and so forth. If the second buffer does not exist, a strictly in-place merge operation must be performed, such as a rotation-based version of the Hwang and Lin algorithm, the Dudzinski and Dydek algorithm, or a repeated binary search and rotate. After dropping the minimum A block and merging the previous A block with the B values that follow it, the new minimum A block must be found within the blocks that are still being rolled through the array. This is handled by running a linear"	"Block sort"
17934404	"search through those A blocks and comparing the tag values to find the smallest one. These remaining A blocks then continue rolling through the array and being dropped and inserted where they belong. This process repeats until all of the A blocks have been dropped and rotated into the previous B block. Once the last remaining A block has been dropped behind and inserted into B where it belongs, it should be merged with the remaining B values that follow it. This completes the merge process for that particular pair of A and B subarrays. However, it must then repeat"	"Block sort"
17934405	"the process for the remaining A and B subarrays for the current level of the merge sort. Note that the internal buffers can be reused for every set of A and B subarrays for this level of the merge sort, and do not need to be re-extracted or modified in any way. After all of the A and B subarrays have been merged, the one or two internal buffers are still left over. The first internal buffer was used for tagging the A blocks, and its contents are still in the same order as before, but the second internal buffer"	"Block sort"
17934406	"may have had its contents rearranged when it was used as swap space for the merges. This means the contents of the second buffer will need to be sorted using a different algorithm, such as insertion sort. The two buffers must then be redistributed back into the array using the opposite process that was used to create them. After repeating these steps for every level of the bottom-up merge sort, the block sort is completed. Block sort works by extracting two internal buffers, breaking A and B subarrays into evenly sized blocks, rolling and dropping the A blocks into B"	"Block sort"
17934407	"(using the first buffer to track the order of the A blocks), locally merging using the second buffer as swap space, sorting the second buffer, and redistributing both buffers. While the steps do not change, these subsystems can vary in their actual implementation. One variant of block sort allows it to use any amount of additional memory provided to it, by using this ""external buffer"" for merging an A subarray or A block with B whenever A fits into it. In this situation it would be identical to a merge sort. Good choices for the buffer size include: Rather than"	"Block sort"
17934408	"tagging the A blocks using the contents of one of the internal buffers, an indirect ""movement-imitation buffer"" can be used instead. This is an internal buffer defined as ""s1 t s2"", where ""s1"" and ""s2"" are each as large as the number of A and B blocks, and ""t"" contains any values immediately following ""s1"" that are equal to the last value of ""s1"" (thus ensuring that no value in ""s2"" appears in ""s1""). A second internal buffer containing unique values is still used. The first values of ""s1"" and ""s2"" are then swapped with each other to encode information"	"Block sort"
17934409	"into the buffer about which blocks are A blocks and which are B blocks. When an A block at index ""i"" is swapped with a B block at index ""j"" (where the first evenly sized A block is initially at index 0), s1[i] and s1[j] are swapped with s2[i] and s2[j], respectively. This ""imitates the movements"" of the A blocks through B. The unique values in the second buffer are used to determine the original order of the A blocks as they are rolled through the B blocks. Once all of the A blocks have been dropped, the movement-imitation buffer"	"Block sort"
17934410	"is used to decode whether a given block in the array is an A block or a B block, each A block is rotated into B, and the second internal buffer is used as swap space for the local merges. The ""second"" value of each A block doesn't necessarily need to be tagged – the first, last, or any other element could be used instead. However, if the first value is tagged, the values will need to be read from the first internal buffer (where they were swapped) when deciding where to drop the minimum A block. Many sorting algorithms"	"Block sort"
17934411	"can be used to sort the contents of the second internal buffer, including unstable sorts like quicksort, since the contents of the buffer are guaranteed to unique. Insertion sort is still recommended, though, for its situational performance and lack of recursion. Block sort is a well-defined and testable class of algorithms, with working implementations available as a merge and as a sort. This allows its characteristics to be measured and considered. Block sort begins by insertion sorting groups of 16–31 items in the array. Insertion sort is an operation, so this leads to anywhere from to , which is once"	"Block sort"
17934412	"the constant factors are omitted. It must also apply an insertion sort on the second internal buffer after each level of merging is completed. However, as this buffer was limited to in size, the operation also ends up being . Next it must extract two internal buffers for each level of the merge sort. It does so by iterating over the items in the A and B subarrays and incrementing a counter whenever the value changes, and upon finding enough values it rotates them to the start of A or the end of B. In the worst case this will"	"Block sort"
17934413	"end up searching the entire array before finding non-contiguous unique values, which requires comparisons and rotations for values. This resolves to , or . When none of the A or B subarrays contained unique values to create the internal buffers, a normally suboptimal in-place merge operation is performed where it repeatedly binary searches and rotates A into B. However, the known lack of unique values within any of the subarrays places a hard limit on the number of binary searches and rotations that will be performed during this step, which is again items rotated up to times, or . The"	"Block sort"
17934414	"size of each block is also adjusted to be smaller in the case where it found unique values but not 2, which further limits the number of unique values contained within any A or B block. Tagging the A blocks is performed times for each A subarray, then the A blocks are rolled through and inserted into the B blocks up to times. The local merges retain the same complexity of a standard merge, albeit with more assignments since the values must be swapped rather than copied. The linear search for finding the new minimum A block iterates over blocks"	"Block sort"
17934415	"times. And the buffer redistribution process is identical to the buffer extraction but in reverse, and therefore has the same complexity. After omitting all but the highest complexity and considering that there are ""log n"" levels in the outer merge loop, this leads to a final asymptotic complexity of for the worst and average cases. For the best case, where the data is already in order, the merge step performs comparisons for the first level, then , , , etc. This is a well-known mathematical series which resolves to . As block sort is non-recursive and does not require the"	"Block sort"
17934416	"use of dynamic allocations, this leads to constant stack and heap space. It uses O(1) auxiliary memory in a transdichotomous model, which accepts that the O(log ""n"") bits needed to keep track of the ranges for A and B cannot be any greater than 32 or 64 on 32-bit or 64-bit computing systems, respectively, and therefore simplifies to O(1) space for any array that can feasibly be allocated. Although items in the array are moved out of order during a block sort, each operation is fully reversible and will have restored the original order of equivalent items by its completion."	"Block sort"
17934417	"Stability requires the first instance of each value in an array before sorting to still be the first instance of that value after sorting. Block sort moves these first instances to the start of the array to create the two internal buffers, but when all of the merges are completed for the current level of the block sort, those values are distributed back to the first sorted position within the array. This maintains stability. Before rolling the A blocks through the B blocks, each A block has its second value swapped with a value from the first buffer. At that"	"Block sort"
17934418	"point the A blocks are moved out of order to roll through the B blocks. However, once it finds where it should insert the smallest A block into the previous B block, that smallest A block is moved back to the start of the A blocks and its second value is restored. By the time all of the A blocks have been inserted, the A blocks will be in order again and the first buffer will contain its original values in the original order. Using the second buffer as swap space when merging an A block with some B values"	"Block sort"
17934419	"causes the contents of that buffer to be rearranged. However, as the algorithm already ensured the buffer only contains unique values, sorting the contents of the buffer is sufficient to restore their original stable order. Block sort is an adaptive sort on two levels: first, it skips merging A and B subarrays that are already in order. Next, when A and B need to be merged and are broken into evenly sized blocks, the A blocks are only rolled through B as far as is necessary, and each block is only merged with the B values immediately following it. The"	"Block sort"
17934420	"more ordered the data originally was, the fewer B values there will be that need to be merged into A. Block sort is a stable sort that does not require additional memory, which is useful in cases where there is not enough free memory to allocate the O(n) buffer. When using the ""external buffer"" variant of block sort, it can scale from using O(n) memory to progressively smaller buffers as needed, and will still work efficiently within those constraints. Block sort does not exploit sorted ranges of data on as fine a level as some other algorithms, such as Timsort."	"Block sort"
17934421	"It only checks for these sorted ranges at the two predefined levels: the A and B subarrays, and the A and B blocks. It is also harder to implement and parallelize compared to a merge sort. Block sort Block sort, or block merge sort, is a sorting algorithm combining at least two merge operations with an insertion sort to arrive at in-place stable sorting. It gets its name from the observation that merging two sorted lists, and , is equivalent to breaking into evenly sized ""blocks"", inserting each block into under special rules, and merging pairs. One practical algorithm for"	"Block sort"
267263	"Merge sort In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and von Neumann as early as 1948. Conceptually, a merge sort works as follows: Example C-like code using indices for top-down merge sort algorithm that recursively splits the list"	"Merge sort"
267264	"(called ""runs"" in this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step is avoided with alternating the direction of the merge with each level of recursion. Example C-like code using indices for bottom-up merge sort algorithm which treats the list as an array of ""n"" sublists (called ""runs"" in this example) of size 1, and iteratively merges sub-lists back and forth between two buffers: Pseudocode for top-down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then"	"Merge sort"
267265	"merges the sublists while returning up the call chain. In this example, the function merges the left and right sublists. Pseudocode for bottom-up merge sort algorithm which uses a small fixed size array of references to nodes, where array[i] is either a reference to a list of size 2 or 0. ""node"" is a reference or pointer to a node. The merge() function would be similar to the one shown in the top-down merge lists example, it merges two already sorted lists, and handles empty lists. In this case, merge() would use ""node"" for its input parameters and return value."	"Merge sort"
267266	"A natural merge sort is similar to a bottom-up merge sort except that any naturally occurring runs (sorted sequences) in the input are exploited. Both monotonic and bitonic (alternating up/down) runs may be exploited, with lists (or equivalently tapes or files) being convenient data structures (used as FIFO queues or LIFO stacks). In the bottom-up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are"	"Merge sort"
267267	"fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. In many practical cases, long natural runs are present, and for that reason natural merge sort is exploited as the key component of Timsort. Example: Tournament replacement selection sorts are used to gather the initial runs for external sorting algorithms. In sorting ""n"" objects, merge sort has an average and worst-case performance of O(""n"" log ""n""). If the running time of merge sort for a list of length ""n"" is"	"Merge sort"
267268	"""T""(""n""), then the recurrence ""T""(""n"") = 2""T""(""n""/2) + ""n"" follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the ""n"" steps taken to merge the resulting two lists). The closed form follows from the master theorem for divide-and-conquer recurrences. In the worst case, the number of comparisons merge sort makes is given by the sorting numbers. These numbers are equal to or slightly smaller than (""n"" ⌈lg ""n""⌉ - 2 + 1), which is between (""n"" lg ""n"" - ""n"" + 1) and (""n"" lg ""n"""	"Merge sort"
267269	"+ ""n"" + O(lg ""n"")). For large ""n"" and a randomly ordered input list, merge sort's expected (average) number of comparisons approaches ""α""·""n"" fewer than the worst case where formula_1 In the ""worst"" case, merge sort does about 39% fewer comparisons than quicksort does in the ""average"" case. In terms of moves, merge sort's worst case complexity is O(""n"" log ""n"")—the same complexity as quicksort's best case, and merge sort's best case takes about half as many iterations as the worst case. Merge sort is more efficient than quicksort for some types of lists if the data to be sorted"	"Merge sort"
267270	"can only be efficiently accessed sequentially, and is thus popular in languages such as Lisp, where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort. Merge sort's most common implementation does not sort in place; therefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for versions that need only ""n""/2 extra spaces). Variants of merge sort are primarily concerned with reducing the space complexity and the cost of copying. A simple alternative for reducing the space overhead to ""n""/2"	"Merge sort"
267271	"is to maintain ""left"" and ""right"" as a combined structure, copy only the ""left"" part of ""m"" into temporary space, and to direct the ""merge"" routine to place the merged output into ""m"". With this version it is better to allocate the temporary space outside the ""merge"" routine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the ""return result"" statement (function "" merge ""in the pseudo code above) become superfluous. One drawback of merge sort, when implemented on arrays, is its working memory requirement. Several in-place"	"Merge sort"
267272	"variants have been suggested: An alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in ""m"" are called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also"	"Merge sort"
267273	"be used. This is a standard sorting technique, not restricted to merge sort. An external merge sort is practical to run using disk or tape drives when the data to be sorted is too large to fit into memory. External sorting explains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just two record buffers and a few program variables. Naming the four tape drives as A, B, C, D, with the"	"Merge sort"
267274	"original data on A, and using only 2 record buffers, the algorithm is similar to Bottom-up implementation, using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows: Instead of starting with very short runs, usually a hybrid algorithm is used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save nine passes. The internal sort is often"	"Merge sort"
267275	"large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory. With some overhead, the above algorithm can be modified to use three tapes. ""O""(""n"" log ""n"") running time can also be achieved using two queues, or a stack and a queue, or three stacks. In the other direction, using ""k"" > two tapes (and ""O""(""k"") items in memory), we can reduce the number of tape operations in ""O""(log ""k"") times by using a k/2-way merge. A more sophisticated merge sort that optimizes tape (and disk) drive"	"Merge sort"
267276	"usage is the polyphase merge sort. On modern computers, locality of reference can be of paramount importance in software optimization, because multilevel memory hierarchies are used. Cache-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the algorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as insertion sort, to discourage"	"Merge sort"
267277	"memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance on machines that benefit from cache optimization. Also, many applications of external sorting use a form of merge sorting where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of pages fit into main memory. Merge sort parallelizes well due to use of the divide-and-conquer method. Several parallel variants are discussed in the third edition of Cormen, Leiserson, Rivest, and Stein's ""Introduction to Algorithms""."	"Merge sort"
267278	"The first of these can be very easily expressed in a pseudocode with fork and join keywords: This algorithm is a trivial modification from the serial version, and its speedup is not impressive: when executed on an infinite number of processors, it runs in time, which is only a improvement on the serial version. A better result can be obtained by using a parallelized merge algorithm, which gives parallelism , meaning that this type of parallel merge sort runs in time if enough processors are available. Such a sort can perform well in practice when combined with a fast stable"	"Merge sort"
267279	"sequential sort, such as insertion sort, and a fast sequential merge as a base case for merging small arrays. Merge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure merge. Other sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in ""O""(log ""n"") time on a CRCW parallel random-access machine (PRAM) with ""n"" processors by performing partitioning implicitly. Powers further"	"Merge sort"
267280	"shows that a pipelined version of Batcher's Bitonic Mergesort at ""O""((log ""n"")) time on a butterfly sorting network is in practice actually faster than his ""O""(log ""n"") sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting. Although heapsort has the same time bounds as merge sort, it requires only Θ(1) auxiliary space instead of merge sort's Θ(""n""). On typical modern architectures, efficient quicksort implementations generally outperform mergesort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media."	"Merge sort"
267281	"Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible. As of Perl 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In Java, the Arrays.sort() methods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch"	"Merge sort"
267282	"to insertion sort when fewer than seven array elements are being sorted. The Linux kernel uses merge sort for its linked lists. Python uses Timsort, another tuned hybrid of merge sort and insertion sort, that has become the standard sort algorithm in Java SE 7 (for arrays of non-primitive types), on the Android platform, and in GNU Octave. Merge sort In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort"	"Merge sort"
954066	"Bucket sort Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort, a generalization of pigeonhole sort, and is a cousin of radix sort in the most-to-least significant digit flavor. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity estimates involve the number of buckets. Bucket sort works as follows: function"	"Bucket sort"
954067	"bucketSort(array, n) is Here ""array"" is the array to be sorted and ""n"" is the number of buckets to use. The function ""msbits(x,k)"" returns the ""k"" most significant bits of ""x"" (""floor(x/2^(size(x)-k))""); different functions can be used to translate the range of elements in ""array"" to ""n"" buckets, such as translating the letters A–Z to 0–25 or returning the first character (0–255) for sorting strings. The function ""nextSort"" is a sorting function; using ""bucketSort"" itself as ""nextSort"" produces a relative of radix sort; in particular, the case ""n = 2"" corresponds to quicksort (although potentially with poor pivot choices). Note"	"Bucket sort"
954068	"that for bucket sort to be formula_1 on average, the number of buckets ""n"" must be equal to the length of the array being sorted, and the input array must be uniformly distributed across the range of possible bucket values. If these requirements are not met, the performance of bucket sort will be dominated by the running time of ""nextSort"", which is typically formula_2 insertion sort, making bucket sort less optimal than formula_3 comparison sort algorithms like Quicksort. A common optimization is to put the unsorted elements of the buckets back in the original array ""first"", then run insertion sort"	"Bucket sort"
954069	"over the complete array; because insertion sort's runtime is based on how far each element is from its final position, the number of comparisons remains relatively small, and the memory hierarchy is better exploited by storing the list contiguously in memory. The most common variant of bucket sort operates on a list of ""n"" numeric inputs between zero and some maximum value ""M"" and divides the value range into ""n"" buckets each of size ""M""/""n"". If each bucket is sorted using insertion sort, the sort can be shown to run in expected linear time (where the average is taken over"	"Bucket sort"
954070	"all possible inputs). However, the performance of this sort degrades with clustering; if many values occur close together, they will all fall into a single bucket and be sorted slowly. This performance degradation is avoided in the original bucket sort algorithm by assuming that the input is generated by a random process that distributes elements uniformly over the interval ""[0,1)"". Since there are ""n"" uniformly distributed elements sorted in to ""n"" buckets the probable number of inputs in each bucket follows a binomial distribution with formula_4 and hence the entire bucket sort will be formula_1 despite the repeated use of"	"Bucket sort"
954071	"formula_2 insertion sort. Similar to generic bucket sort as described above, ProxmapSort works by dividing an array of keys into subarrays via the use of a ""map key"" function that preserves a partial ordering on the keys; as each key is added to its subarray, insertion sort is used to keep that subarray sorted, resulting in the entire array being in sorted order when ProxmapSort completes. ProxmapSort differs from bucket sorts in its use of the map key to place the data approximately where it belongs in sorted order, producing a ""proxmap"" — a proximity mapping — of the keys."	"Bucket sort"
954072	"Another variant of bucket sort known as histogram sort or counting sort adds an initial pass that counts the number of elements that will fall into each bucket using a count array. Using this information, the array values can be arranged into a sequence of buckets in-place by a sequence of exchanges, leaving no space overhead for bucket storage. The Postman's sort is a variant of bucket sort that takes advantage of a hierarchical structure of elements, typically described by a set of attributes. This is the algorithm used by letter-sorting machines in post offices: mail is sorted first between"	"Bucket sort"
954073	"domestic and international; then by state, province or territory; then by destination post office; then by routes, etc. Since keys are not compared against each other, sorting time is O(""cn""), where ""c"" depends on the size of the key and number of buckets. This is similar to a radix sort that works ""top down,"" or ""most significant digit first."" The shuffle sort is a variant of bucket sort that begins by removing the first 1/8 of the ""n"" items to be sorted, sorts them recursively, and puts them in an array. This creates ""n""/8 ""buckets"" to which the remaining 7/8"	"Bucket sort"
954074	"of the items are distributed. Each ""bucket"" is then sorted, and the ""buckets"" are concatenated into a sorted array. Bucket sort can be seen as a generalization of counting sort; in fact, if each bucket has size 1 then bucket sort degenerates to counting sort. The variable bucket size of bucket sort allows it to use O(""n"") memory instead of O(""M"") memory, where ""M"" is the number of distinct values; in exchange, it gives up counting sort's O(""n"" + ""M"") worst-case behavior. Bucket sort with two buckets is effectively a version of quicksort where the pivot value is always selected"	"Bucket sort"
954075	"to be the middle value of the value range. While this choice is effective for uniformly distributed inputs, other means of choosing the pivot in quicksort such as randomly selected pivots make it more resistant to clustering in the input distribution. The ""n""-way mergesort algorithm also begins by distributing the list into ""n"" sublists and sorting each one; however, the sublists created by mergesort have overlapping value ranges and so cannot be recombined by simple concatenation as in bucket sort. Instead, they must be interleaved by a merge algorithm. However, this added expense is counterbalanced by the simpler scatter phase"	"Bucket sort"
954076	"and the ability to ensure that each sublist is the same size, providing a good worst-case time bound. Top-down radix sort can be seen as a special case of bucket sort where both the range of values and the number of buckets is constrained to be a power of two. Consequently, each bucket's size is also a power of two, and the procedure can be applied recursively. This approach can accelerate the scatter phase, since we only need to examine a prefix of the bit representation of each element to determine its bucket. Bucket sort Bucket sort, or bin sort,"	"Bucket sort"
1389690	"Comb sort Comb sort is a relatively simple sorting algorithm originally designed by Włodzimierz Dobosiewicz in 1980. Later it was rediscovered by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort. The basic idea is to eliminate ""turtles"", or small values near the end of the list, since in a bubble sort these slow the sorting down tremendously. ""Rabbits"", large values around the beginning of the list, do not pose a problem in bubble sort. In bubble sort, when any two elements are compared, they always have a ""gap"" (distance from each other) of 1. The"	"Comb sort"
1389691	"basic idea of comb sort is that the gap can be much more than 1. The inner loop of bubble sort, which does the actual ""swap"", is modified such that gap between swapped elements goes down (for each iteration of outer loop) in steps of a ""shrink factor"" ""k"": [ ""n""/""k"", ""n""/""k"", ""n""/""k"", ..., 1 ]. The gap starts out as the length of the list ""n"" being sorted divided by the shrink factor ""k"" (generally 1.3; see below) and one pass of the aforementioned modified bubble sort is applied with that gap. Then the gap is divided by the"	"Comb sort"
1389692	"shrink factor again, the list is sorted with this new gap, and the process repeats until the gap is 1. At this point, comb sort continues using a gap of 1 until the list is fully sorted. The final stage of the sort is thus equivalent to a bubble sort, but by this time most turtles have been dealt with, so a bubble sort will be efficient. The shrink factor has a great effect on the efficiency of comb sort. ""k"" = 1.3 has been suggested as an ideal shrink factor by the authors of the original article after empirical"	"Comb sort"
1389693	"testing on over 200,000 random lists. A value too small slows the algorithm down by making unnecessarily many comparisons, whereas a value too large fails to effectively deal with turtles, making it require many passes with 1 gap size. The pattern of repeated sorting passes with decreasing gaps is similar to Shellsort, but in Shellsort the array is sorted completely each pass before going on to the next-smallest gap. Comb sort's passes do not completely sort the elements. This is the reason that Shellsort gap sequences have a larger optimal shrink factor of about 2.2. function combsort(array input) Comb sort"	"Comb sort"
278025	"Merge algorithm Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms, most famously merge sort. The merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm. Conceptually, merge sort algorithm consists of two steps: The merge algorithm is used repeatedly in the merge sort algorithm. An example merge sort is given in the illustration. It starts with an unsorted array of 7"	"Merge algorithm"
278026	"integers. The array is divided into 7 partitions; each partition contains 1 element and is sorted. The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left. Merging two sorted lists into one can be done in linear time and linear space. The following pseudocode demonstrates an algorithm that merges input lists (either linked lists or arrays) and into a new list . The function yields the first element of a list; ""dropping"" an element means removing it from its list, typically by incrementing a pointer or index. When the inputs are"	"Merge algorithm"
278027	"linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list. In the merge sort algorithm, this subroutine is typically used to merge two sub-arrays , of a single array . This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above. The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised, sometimes sacrificing the"	"Merge algorithm"
278028	"linear-time bound to produce an algorithm; see for discussion. -way merging generalizes binary merging to an arbitrary number of sorted input lists. Applications of -way merging arise in various sorting algorithms, including patience sorting and an external sorting algorithm that divides its input into blocks that fit in memory, sorts these one by one, then merges these blocks. Several solutions to this problem exist. A naive solution is to do a loop over the lists to pick off the minimum element each time, and repeat this loop until all lists are empty: In the worst case, this algorithm performs element"	"Merge algorithm"
278029	"comparisons to perform its work if there are a total of elements in the lists. It can be improved by storing the lists in a priority queue (min-heap) keyed by their first element: Searching for the next smallest element to be output (find-min) and restoring heap order can now be done in time (more specifically, comparisons), and the full problem can be solved in time (approximately comparisons). A third algorithm for the problem is a divide and conquer solution that builds on the binary merge algorithm: When the input lists to this algorithm are ordered by length, shortest first, it"	"Merge algorithm"
278030	"requires fewer than comparisons, i.e., less than half the number used by the heap-based algorithm; in practice, it may be about as fast or slow as the heap-based algorithm. A parallel version of the binary merge algorithm can serve as a building block of a parallel merge sort. The following pseudocode demonstrates this algorithm in a parallel divide-and-conquer style (adapted from Cormen ""et al.""). It operates on two sorted arrays and and writes the sorted output to array . The notation denotes the part of from index through , exclusive. The algorithm operates by splitting either or , whichever is"	"Merge algorithm"
278031	"larger, into (nearly) equal halves. It then splits the other array into a part that is smaller than the midpoint of the first, and a part that is larger. (The binary search subroutine returns the index in where would be, if it were in ; that this always a number between and .) Finally, each pair of halves is merged recursively, and since the recursive calls are independent of each other, they can be done in parallel. Hybrid approach, where serial algorithm is used for recursion base case has been shown to perform well in practice The work performed by"	"Merge algorithm"
278032	"the algorithm for two arrays holding a total of elements, i.e., the running time of a serial version of it, is . This is optimal since elements need to be copied into . Its critical path length, however, is , meaning that it takes that much time on an ideal machine with an unbounded number of processors. Note: The routine is not stable: if equal items are separated by splitting and , they will become interleaved in ; also swapping and will destroy the order, if equal items are spread among both input arrays. As a result, when used for"	"Merge algorithm"
278033	"sorting, this algorithm produces a sort that is not stable. Some computer languages provide built-in or library support for merging sorted collections. The C++'s Standard Template Library has the function , which merges two sorted ranges of iterators, and , which merges two consecutive sorted ranges ""in-place"". In addition, the (linked list) class has its own method which merges another list into itself. The type of the elements merged must support the less-than () operator, or it must be provided with a custom comparator. C++17 allows for differing execution policies, namely sequential, parallel, and parallel-unsequenced. Python's standard library (since 2.6)"	"Merge algorithm"
278034	"also has a function in the module, that takes multiple sorted iterables, and merges them into a single iterator. Merge algorithm Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms, most famously merge sort. The merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm. Conceptually, merge sort algorithm consists of two steps: The merge algorithm is used repeatedly in the merge"	"Merge algorithm"
6060359	"Bitonic sorter Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of formula_1 comparators and have a delay of formula_2, where formula_3 is the number of items to be sorted. A sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A ""bitonic"" sequence is a sequence with formula_4 for some formula_5, or a circular shift of such a sequence. The following is a bitonic sorting network with 16 inputs: The 16 numbers enter at the inputs"	"Bitonic sorter"
6060360	"at the left end, slide along each of the 16 horizontal wires, and exit at the outputs at the right end. The network is designed to sort the elements, with the largest number at the bottom. The arrows are comparators. Whenever two numbers reach the two ends of an arrow, they are compared to ensure that the arrow points toward the larger number. If they are out of order, they are swapped. The colored boxes are just for illustration and have no effect on the algorithm. Every red box has the same structure: each input in the top half is"	"Bitonic sorter"
6060361	"compared to the corresponding input in the bottom half, with all arrows pointing down (dark red) or all up (light red). If the inputs happen to form a bitonic sequence (a single nondecreasing sequence followed by a single nonincreasing one or vice versa), then the output will form two bitonic sequences. The top half of the output will be bitonic, and the bottom half will be bitonic, with every element of the top half less than or equal to every element of the bottom half (for dark red) or vice versa (for light red). This theorem is not obvious, but"	"Bitonic sorter"
6060362	"can be verified by carefully considering all the cases of how the various inputs might compare, using the zero-one principle, where a bitonic sequence is a sequence of 0s and 1s that contains no more than two ""10"" or ""01"" subsequences. The red boxes combine to form blue and green boxes. Every such box has the same structure: a red box is applied to the entire input sequence, then to each half of the result, then to each half of each of those results, and so on. All arrows point down (blue) or all point up (green). This structure is"	"Bitonic sorter"
6060363	"known as a butterfly network. If the input to this box happens to be bitonic, then the output will be completely sorted in increasing order (blue) or decreasing order (green). If a number enters the blue or green box, then the first red box will sort it into the correct half of the list. It will then pass through a smaller red box that sorts it into the correct quarter of the list within that half. This continues until it is sorted into exactly the correct position. Therefore, the output of the green or blue box will be completely sorted."	"Bitonic sorter"
6060364	"The green and blue boxes combine to form the entire sorting network. For any arbitrary sequence of inputs, it will sort them correctly, with the largest at the bottom. The output of each green or blue box will be a sorted sequence, so the output of each pair of adjacent lists will be bitonic, because the top one is blue and the bottom one is green. Each column of blue and green boxes takes N sorted sequences and concatenates them in pairs to form N/2 bitonic sequences, which are then sorted by the boxes in that column to form N/2"	"Bitonic sorter"
6060365	"sorted sequences. This process starts with each input considered to be a sorted list of one element, and continues through all the columns until the last merges them into a single, sorted list. Because the last stage was blue, this final list will have the largest element at the bottom. Each green box performs the same operation as a blue box, but with the sort in the opposite direction. So, each green box could be replaced by a blue box followed by a crossover where all the wires move to the opposite position. This would allow all the arrows to"	"Bitonic sorter"
6060366	"point the same direction, but would prevent the horizontal lines from being straight. However, a similar crossover could be placed to the right of the bottom half of the outputs from any red block, and the sort would still work correctly, because the reverse of a bitonic sequence is still bitonic. If a red box then has a crossover before and after it, it can be rearranged internally so the two crossovers cancel, so the wires become straight again. Therefore, the following diagram is equivalent to the one above, where each green box has become a blue plus a crossover,"	"Bitonic sorter"
6060367	"and each orange box is a red box that absorbed two such crossovers: The arrowheads are not drawn, because every comparator sorts in the same direction. The blue and red blocks perform the same operations as before. The orange blocks are equivalent to red blocks where the sequence order is reversed for the bottom half of its inputs and the bottom half of its outputs. This is the most common representation of a bitonic sorting network The following is an implementation of the bitonic mergesort sorting algorithm in Python. The input is a boolean value ""up"", and a list ""x"""	"Bitonic sorter"
6060368	"of length a power of 2. The output is a sorted list that is ascending if ""up"" is true, and decreasing otherwise. The following is another implementation in Java. Bitonic sorter Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of formula_1 comparators and have a delay of formula_2, where formula_3 is the number of items to be sorted. A sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A ""bitonic"" sequence is a sequence with"	"Bitonic sorter"
2289113	"Introsort Introsort or introspective sort is a hybrid sorting algorithm that provides both fast average performance and (asymptotically) optimal worst-case performance. It begins with quicksort and switches to heapsort when the recursion depth exceeds a level based on (the logarithm of) the number of elements being sorted. This combines the good parts of both algorithms, with practical performance comparable to quicksort on typical data sets and worst-case O(""n"" log ""n"") runtime due to the heap sort. Since both algorithms it uses are comparison sorts, it is also a comparison sort. Introsort was invented by David Musser in , in which"	Introsort
2289114	"he also introduced introselect, a hybrid selection algorithm based on quickselect (a variant of quicksort), which falls back to median of medians and thus provides worst-case linear complexity, which is optimal. Both algorithms were introduced with the purpose of providing generic algorithms for the C++ Standard Library which had both fast average performance and optimal worst-case performance, thus allowing the performance requirements to be tightened. If a heapsort implementation and partitioning functions of the type discussed in the quicksort article are available, the introsort can be described succinctly as The factor two in the maximum depth is arbitrary; it can"	Introsort
2289115	"be tuned for practical performance. denotes the array slice of items to . In quicksort, one of the critical operations is choosing the pivot: the element around which the list is partitioned. The simplest pivot selection algorithm is to take the first or the last element of the list as the pivot, causing poor behavior for the case of sorted or nearly sorted input. Niklaus Wirth's variant uses the middle element to prevent these occurrences, degenerating to O(""n"") for contrived sequences. The median-of-3 pivot selection algorithm takes the median of the first, middle, and last elements of the list; however,"	Introsort
2289116	"even though this performs well on many real-world inputs, it is still possible to contrive a ""median-of-3 killer"" list that will cause dramatic slowdown of a quicksort based on this pivot selection technique. Musser reported that on a median-of-3 killer sequence of 100,000 elements, introsort's running time was 1/200 that of median-of-3 quicksort. Musser also considered the effect on caches of Sedgewick's delayed small sorting, where small ranges are sorted at the end in a single pass of insertion sort. He reported that it could double the number of cache misses, but that its performance with double-ended queues was significantly"	Introsort
2289117	"better and should be retained for template libraries, in part because the gain in other cases from doing the sorts immediately was not great. Introsort or some variant is used in a number of standard library sort functions, including some C++ sort implementations. The June 2000 SGI C++ Standard Template Library stl_algo.h implementation of unstable sort uses the Musser introsort approach with the recursion depth to switch to heapsort passed as a parameter, median-of-3 pivot selection and the Knuth final insertion sort pass for partitions smaller than 16. The GNU Standard C++ library is similar: uses introsort with a maximum"	Introsort
2289118	"depth of 2×log ""n"", followed by an insertion sort on partitions smaller than 16. The Microsoft .NET Framework Class Library, starting from version 4.5 (2012), uses Introsort instead of simple QuickSort. Introsort Introsort or introspective sort is a hybrid sorting algorithm that provides both fast average performance and (asymptotically) optimal worst-case performance. It begins with quicksort and switches to heapsort when the recursion depth exceeds a level based on (the logarithm of) the number of elements being sorted. This combines the good parts of both algorithms, with practical performance comparable to quicksort on typical data sets and worst-case O(""n"" log"	Introsort
